{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94f62980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/billy/code/jacksharples1/youtube_optimizer/nlp_bw_folder/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871e5530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(744, 5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "df=pd.read_csv('/Users/billy/code/jacksharples1/youtube_optimizer/starter_title_train.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8517da30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>xL0ch83RAK8</td>\n",
       "      <td>UC8ROUUjHzEQm-ndb69CX8Ww</td>\n",
       "      <td>Âè∞Ë¶ñÊñ∞ËÅûÂè∞HD 24 Â∞èÊôÇÁ∑ö‰∏äÁõ¥Êí≠ÔΩúTAIWAN TTV NEWS HD (Live)ÔΩúÂè∞Êπæ...</td>\n",
       "      <td>106950818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>JOlc_ksXncM</td>\n",
       "      <td>UCY64_tffZ71RniyKDj5bWRA</td>\n",
       "      <td>Crispy French Fries Recipe! You Can Make PERFE...</td>\n",
       "      <td>1340138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>doqBgKIBBh8</td>\n",
       "      <td>UCUaLy1_4rsLo4HyCLCPlD4g</td>\n",
       "      <td>üî¥ LIVE WEBCAM from LANZAROTE AIRPORT</td>\n",
       "      <td>18880210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>PM4JQE-35O4</td>\n",
       "      <td>UCDfYcuNw-ff952Xn0VDWwDw</td>\n",
       "      <td>SO WE GOT BANDICOOT-NAPPED... || Crash Bandico...</td>\n",
       "      <td>90917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>gBifepJRAXU</td>\n",
       "      <td>UCDfYcuNw-ff952Xn0VDWwDw</td>\n",
       "      <td>N.GIN HAS MADE HIS OWN ANIMATRONIC? || Crash B...</td>\n",
       "      <td>101879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0</td>\n",
       "      <td>hs379lua3gY</td>\n",
       "      <td>UC9lRSat3J2IhZ0_TOV-XNzA</td>\n",
       "      <td>Big Update Tomorrow?!</td>\n",
       "      <td>159562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>0</td>\n",
       "      <td>I7cWQMxvBg8</td>\n",
       "      <td>UC8YFOP87ktce_6EqAmtzOqQ</td>\n",
       "      <td>Magnetic Anti Slip Drill Bit</td>\n",
       "      <td>8319404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0</td>\n",
       "      <td>79OJZbCnqCE</td>\n",
       "      <td>UCQrWS87-KWTTEpEl7sp_KjA</td>\n",
       "      <td>Alguien ha cobrado por escribir esto - Caith C...</td>\n",
       "      <td>9966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>0</td>\n",
       "      <td>z8TUNvRs1KU</td>\n",
       "      <td>UCTUO_3bYr50vmH0aE7byMjg</td>\n",
       "      <td>Icefrog doesn&amp;#39;t want you to know this</td>\n",
       "      <td>190248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>0</td>\n",
       "      <td>qV3rmYKnsTc</td>\n",
       "      <td>UCSn0_ckKBvHeZLokQjjVXww</td>\n",
       "      <td>GTA 5 ONLINE üê∑ MEGA RAMPE PERICOLOSE !!!üê∑ GARE...</td>\n",
       "      <td>193266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>565 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0           id                channel_id  \\\n",
       "0             0  xL0ch83RAK8  UC8ROUUjHzEQm-ndb69CX8Ww   \n",
       "1             0  JOlc_ksXncM  UCY64_tffZ71RniyKDj5bWRA   \n",
       "2             0  doqBgKIBBh8  UCUaLy1_4rsLo4HyCLCPlD4g   \n",
       "3             0  PM4JQE-35O4  UCDfYcuNw-ff952Xn0VDWwDw   \n",
       "4             0  gBifepJRAXU  UCDfYcuNw-ff952Xn0VDWwDw   \n",
       "..          ...          ...                       ...   \n",
       "739           0  hs379lua3gY  UC9lRSat3J2IhZ0_TOV-XNzA   \n",
       "740           0  I7cWQMxvBg8  UC8YFOP87ktce_6EqAmtzOqQ   \n",
       "741           0  79OJZbCnqCE  UCQrWS87-KWTTEpEl7sp_KjA   \n",
       "742           0  z8TUNvRs1KU  UCTUO_3bYr50vmH0aE7byMjg   \n",
       "743           0  qV3rmYKnsTc  UCSn0_ckKBvHeZLokQjjVXww   \n",
       "\n",
       "                                                 title      views  \n",
       "0    Âè∞Ë¶ñÊñ∞ËÅûÂè∞HD 24 Â∞èÊôÇÁ∑ö‰∏äÁõ¥Êí≠ÔΩúTAIWAN TTV NEWS HD (Live)ÔΩúÂè∞Êπæ...  106950818  \n",
       "1    Crispy French Fries Recipe! You Can Make PERFE...    1340138  \n",
       "2                 üî¥ LIVE WEBCAM from LANZAROTE AIRPORT   18880210  \n",
       "3    SO WE GOT BANDICOOT-NAPPED... || Crash Bandico...      90917  \n",
       "4    N.GIN HAS MADE HIS OWN ANIMATRONIC? || Crash B...     101879  \n",
       "..                                                 ...        ...  \n",
       "739                              Big Update Tomorrow?!     159562  \n",
       "740                       Magnetic Anti Slip Drill Bit    8319404  \n",
       "741  Alguien ha cobrado por escribir esto - Caith C...       9966  \n",
       "742          Icefrog doesn&#39;t want you to know this     190248  \n",
       "743  GTA 5 ONLINE üê∑ MEGA RAMPE PERICOLOSE !!!üê∑ GARE...     193266  \n",
       "\n",
       "[565 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af4283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2646a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['detect'] = df['title'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a198d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.title[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c19847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_english.title[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "734f46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = df[df['detect'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d111045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>detect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>JOlc_ksXncM</td>\n",
       "      <td>UCY64_tffZ71RniyKDj5bWRA</td>\n",
       "      <td>Crispy French Fries Recipe! You Can Make PERFE...</td>\n",
       "      <td>1340138</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>doqBgKIBBh8</td>\n",
       "      <td>UCUaLy1_4rsLo4HyCLCPlD4g</td>\n",
       "      <td>üî¥ LIVE WEBCAM from LANZAROTE AIRPORT</td>\n",
       "      <td>18880210</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>PM4JQE-35O4</td>\n",
       "      <td>UCDfYcuNw-ff952Xn0VDWwDw</td>\n",
       "      <td>SO WE GOT BANDICOOT-NAPPED... || Crash Bandico...</td>\n",
       "      <td>90917</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>gBifepJRAXU</td>\n",
       "      <td>UCDfYcuNw-ff952Xn0VDWwDw</td>\n",
       "      <td>N.GIN HAS MADE HIS OWN ANIMATRONIC? || Crash B...</td>\n",
       "      <td>101879</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>8KGXSjtS5pk</td>\n",
       "      <td>UCMpn1qLudF-zb4M4bqxLIbw</td>\n",
       "      <td>üî¥Live Webcam around the World - Live Camera - ...</td>\n",
       "      <td>4838120</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0</td>\n",
       "      <td>G3KsFO7GCR0</td>\n",
       "      <td>UCgaiUkHAOhG6ynlboA8YAaA</td>\n",
       "      <td>Despicable Me (2010) Final Battle with healthbars</td>\n",
       "      <td>613227</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>0</td>\n",
       "      <td>DVwq_vaKGtA</td>\n",
       "      <td>UCst9GLZ-X47MxWBmx9cCrKA</td>\n",
       "      <td>xQc Shows Me His New $1,000,000 Lambo in GTA 5 RP</td>\n",
       "      <td>215226</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0</td>\n",
       "      <td>hs379lua3gY</td>\n",
       "      <td>UC9lRSat3J2IhZ0_TOV-XNzA</td>\n",
       "      <td>Big Update Tomorrow?!</td>\n",
       "      <td>159562</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>0</td>\n",
       "      <td>z8TUNvRs1KU</td>\n",
       "      <td>UCTUO_3bYr50vmH0aE7byMjg</td>\n",
       "      <td>Icefrog doesn&amp;#39;t want you to know this</td>\n",
       "      <td>190248</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>0</td>\n",
       "      <td>qV3rmYKnsTc</td>\n",
       "      <td>UCSn0_ckKBvHeZLokQjjVXww</td>\n",
       "      <td>GTA 5 ONLINE üê∑ MEGA RAMPE PERICOLOSE !!!üê∑ GARE...</td>\n",
       "      <td>193266</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>314 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0           id                channel_id  \\\n",
       "1             0  JOlc_ksXncM  UCY64_tffZ71RniyKDj5bWRA   \n",
       "2             0  doqBgKIBBh8  UCUaLy1_4rsLo4HyCLCPlD4g   \n",
       "3             0  PM4JQE-35O4  UCDfYcuNw-ff952Xn0VDWwDw   \n",
       "4             0  gBifepJRAXU  UCDfYcuNw-ff952Xn0VDWwDw   \n",
       "7             0  8KGXSjtS5pk  UCMpn1qLudF-zb4M4bqxLIbw   \n",
       "..          ...          ...                       ...   \n",
       "735           0  G3KsFO7GCR0  UCgaiUkHAOhG6ynlboA8YAaA   \n",
       "737           0  DVwq_vaKGtA  UCst9GLZ-X47MxWBmx9cCrKA   \n",
       "739           0  hs379lua3gY  UC9lRSat3J2IhZ0_TOV-XNzA   \n",
       "742           0  z8TUNvRs1KU  UCTUO_3bYr50vmH0aE7byMjg   \n",
       "743           0  qV3rmYKnsTc  UCSn0_ckKBvHeZLokQjjVXww   \n",
       "\n",
       "                                                 title     views detect  \n",
       "1    Crispy French Fries Recipe! You Can Make PERFE...   1340138     en  \n",
       "2                 üî¥ LIVE WEBCAM from LANZAROTE AIRPORT  18880210     en  \n",
       "3    SO WE GOT BANDICOOT-NAPPED... || Crash Bandico...     90917     en  \n",
       "4    N.GIN HAS MADE HIS OWN ANIMATRONIC? || Crash B...    101879     en  \n",
       "7    üî¥Live Webcam around the World - Live Camera - ...   4838120     en  \n",
       "..                                                 ...       ...    ...  \n",
       "735  Despicable Me (2010) Final Battle with healthbars    613227     en  \n",
       "737  xQc Shows Me His New $1,000,000 Lambo in GTA 5 RP    215226     en  \n",
       "739                              Big Update Tomorrow?!    159562     en  \n",
       "742          Icefrog doesn&#39;t want you to know this    190248     en  \n",
       "743  GTA 5 ONLINE üê∑ MEGA RAMPE PERICOLOSE !!!üê∑ GARE...    193266     en  \n",
       "\n",
       "[314 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150636ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itranslate import itranslate as itrans\n",
    "# df_test['title_translated'] = df_test['title'].apply(lambda x: itrans(x, to_lang='en'))\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cfe1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['detect'] != 'en'].title.apply(lambda x: itrans(x, to_lang='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec0d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6354a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644d194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c873d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/6snstltx42x6_ysnql018md00000gn/T/ipykernel_66023/3187740504.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english.drop(columns=['Unnamed: 0', 'id', 'channel_id', 'detect'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_english.drop(columns=['Unnamed: 0', 'id', 'channel_id', 'detect'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c38b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string # \"string\" module is already installed with Python\n",
    "string.punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db9d5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def emoji_strip(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text) # no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af927953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "#     Lower\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "#     punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "\n",
    "#   no numbers\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "\n",
    "    sentence = emoji_strip(sentence)\n",
    "\n",
    "    clean_titles = lemmatize(sentence)\n",
    "\n",
    "    return clean_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce84ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    # Lemmatizing the verbs\n",
    "    verb_lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "        for word in text\n",
    "    ]\n",
    "\n",
    "# 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "        for word in text\n",
    "    ]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7f073cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_english.title.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52d881b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/6snstltx42x6_ysnql018md00000gn/T/ipykernel_66023/1494558301.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_english.title = df.title.apply(preprocessing)\n"
     ]
    }
   ],
   "source": [
    "df_english.title = df.title.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da89ef45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crispy french fries recipe you can make perfec...</td>\n",
       "      <td>1340138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>live webcam from lanzarote airport</td>\n",
       "      <td>18880210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so we got bandicootnapped  crash bandicoot  part</td>\n",
       "      <td>90917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ngin has made his own animatronic  crash bandi...</td>\n",
       "      <td>101879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>live webcam around the world  live camera  c√°m...</td>\n",
       "      <td>4838120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title     views\n",
       "1  crispy french fries recipe you can make perfec...   1340138\n",
       "2                 live webcam from lanzarote airport  18880210\n",
       "3  so we got bandicootnapped  crash bandicoot  part      90917\n",
       "4  ngin has made his own animatronic  crash bandi...    101879\n",
       "7  live webcam around the world  live camera  c√°m...   4838120"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92d93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0822502d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'views'],\n",
       "        num_rows: 219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'views'],\n",
       "        num_rows: 95\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df_english,preserve_index=False) \n",
    "dataset = dataset.train_test_split(test_size=0.3) \n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baa0cd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['train']['title'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db832546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aaf5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing(df_english.title[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4366aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bdcec9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2634,\n",
       " 9189,\n",
       " 2444,\n",
       " 2634,\n",
       " 2694,\n",
       " 20979,\n",
       " 11431,\n",
       " 2158,\n",
       " 2818,\n",
       " 2444,\n",
       " 2938,\n",
       " 19524,\n",
       " 17136,\n",
       " 2678,\n",
       " 14288,\n",
       " 2602,\n",
       " 4552,\n",
       " 2154,\n",
       " 102]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(df_english.title[42])['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c3b699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [tokenizer.decode(i) for i in tokenizer(df_english.title[42])['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "334e7523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ['üö®', 'üôÇ', 'üòç', '‚úåÔ∏è' , 'ü§© ']:\n",
    "#     tokenizer.add_tokens(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54b84178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [tokenizer.decode(i) for i in tokenizer(df_english.title[42])['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebe84b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a4492338e34b1aacb5720c4de1c6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3f1417c9c9433bacd08f3a4f00d79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7556621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b71bb922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b44bd61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33bf7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95d477c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='mae', metrics=['mse', 'mae', 'rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45f7fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# es= EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4b715d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: views, title. If views, title are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/billy/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 219\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 42\n",
      "  Number of trainable parameters = 66954241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [35], line 30\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# train = tokenized_datasets['train']\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# test = tokenized_datasets['test']\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# model.fit(train, batch_size=5, epochs=8, callbacks=es)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     23\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     24\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2508\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2511\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/trainer.py:2553\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m-> 2553\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2556\u001b[0m         )\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2558\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  logging_strategy=\"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  per_device_eval_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  save_total_limit = 2,\n",
    "                                  save_strategy = 'no',\n",
    "                                  load_best_model_at_end=False\n",
    "                                  )\n",
    "\n",
    "\n",
    "# train = tokenized_datasets['train']\n",
    "\n",
    "# test = tokenized_datasets['test']\n",
    "\n",
    "\n",
    "# model.fit(train, batch_size=5, epochs=8, callbacks=es)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e58e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model/tokenizer\n",
    "model.save_pretrained(\"model\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "# load the model/tokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True) \n",
    "def pipeline_prediction(text):\n",
    "    df=pd.DataFrame({'text':[text]})\n",
    "    dataset = Dataset.from_pandas(df,preserve_index=False) \n",
    "    tokenized_datasets = dataset.map(tokenize_function)\n",
    "    raw_pred, _, _ = trainer.predict(tokenized_datasets) \n",
    "    return(raw_pred[0][0])\n",
    "pipeline_prediction(\"üö® Get 50% now!\")\n",
    "-0.019468416"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7500aecb59e023174419455051a5cb7b1fe8bc20dea22faf86cb322f95ff8ced"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
