{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c72dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string # \"string\" module is already installed with Python\n",
    "string.punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "import pandas as pd\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968688af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('big_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc52ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def emoji_strip(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text) # no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e633c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "#     Lower\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "#     punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "\n",
    "#   no numbers\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "\n",
    "    sentence = emoji_strip(sentence)\n",
    "\n",
    "    clean_titles = lemmatize(sentence)\n",
    "\n",
    "    return clean_titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    # Lemmatizing the verbs\n",
    "    verb_lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "        for word in text\n",
    "    ]\n",
    "\n",
    "# 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "        for word in text\n",
    "    ]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb91c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title = df.title.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = {}\n",
    "basic_sent_dict={}\n",
    "for i in df.title:\n",
    "    emotion = NRCLex(i)\n",
    "    # print(f'TITLE: {i}\\n')\n",
    "    # print(f'affect dictionary: {emotion.affect_dict}\\n')\n",
    "    # print(f'words: {emotion.words}\\n')\n",
    "    # print(f'sentences: {emotion.sentences}\\n')\n",
    "    # print(f'affect list: {emotion.affect_list}\\n')\n",
    "    # print(f'raw emotion score: {emotion.raw_emotion_scores}\\n')\n",
    "    # print(f'top emotions: {emotion.top_emotions}\\n')\n",
    "    # print(f'affect frequency: {emotion.affect_frequencies}\\n')\n",
    "    \n",
    "    \n",
    "# advanced sentiment dictionary\n",
    "    sent_dict[i] = emotion.affect_frequencies\n",
    "    \n",
    "# basic sentiment dictionary\n",
    "    if 'negative' in emotion.raw_emotion_scores:\n",
    "        basic_sent_dict[i] = 0\n",
    "    elif 'positive' in emotion.raw_emotion_scores:\n",
    "        basic_sent_dict[i] = 1\n",
    "    else:\n",
    "        basic_sent_dict[i] = 0.5\n",
    "\n",
    "# merge basic sent df\n",
    "df_basic_sent = pd.DataFrame.from_dict(basic_sent_dict, orient = \"index\", columns = ['sentiment']).reset_index()\n",
    "\n",
    "df_basic_sent.rename(columns = {'index':'title'}, inplace = True)\n",
    "\n",
    "df = df.merge(df_basic_sent, on='title')\n",
    "\n",
    "# merge sentiment df\n",
    "df_sent = pd.DataFrame.from_dict(sent_dict,\n",
    "                                 orient = \"index\",\n",
    "                                 columns = ['anger',\n",
    "                                            'anticip',\n",
    "                                            'trust', \n",
    "                                            'surprise', \n",
    "                                            'positive', \n",
    "                                            'negative', \n",
    "                                            'sadness', \n",
    "                                            'disgust',\n",
    "                                            'joy']).reset_index()\n",
    "\n",
    "df_sent.rename(columns = {'index':'title'}, inplace = True)\n",
    "\n",
    "df = df.merge(df_sent, on='title')\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea59fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(round(df.corr(),3), cmap = \"coolwarm\", annot = True, annot_kws = {\"size\":10});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['negative'] != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294dbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['positive'] != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a54191",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_ratio = (df['negative'] != 0).sum()/df.shape[0]\n",
    "print(f'Negative ratio is {negative_ratio*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111e982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_ratio = (df['positive'] != 0).sum()/df.shape[0]\n",
    "print(f'Positive ratio is {positive_ratio*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b041a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_ratio = (df.shape[0] - ((df['negative'] != 0).sum() + (df['positive'] != 0).sum()))/df.shape[0]\n",
    "print(f'Neutral ratio is {neutral_ratio*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ratio = (df['negative'] != 0).sum()/df.shape[0]\n",
    "print(f'Negative ratio is {negative_ratio*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # declaring data\n",
    "data = [(df['anger'] != 0).sum(), (df['anticip'] != 0).sum(),\n",
    "        (df['trust'] != 0).sum(), (df['surprise'] != 0).sum(),\n",
    "        (df['sadness'] != 0).sum(), (df['disgust'] != 0).sum(),\n",
    "        (df['joy'] != 0).sum()]\n",
    "\n",
    "keys = ['Anger', 'Anticipation', 'Trust', 'Surprise', 'Sadness', 'Disgust', 'Joy']\n",
    "  \n",
    "# declaring exploding pie\n",
    "# explode = [0, 0.1, 0, 0, 0]\n",
    "# define Seaborn color palette to use\n",
    "palette_color = sns.color_palette('dark')\n",
    "  \n",
    "# plotting data on chart\n",
    "plt.pie(data, labels=keys, colors=palette_color, autopct='%.0f%%')\n",
    "  \n",
    "# displaying chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5062b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.sentiment);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high = df[df.views > 200000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f602c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df_high, x='sentiment', y='views')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "\n",
    "def kendall_pval(x,y):\n",
    "    return kendalltau(x,y)[1]\n",
    "\n",
    "def pearsonr_pval(x,y):\n",
    "    return pearsonr(x,y)[1]\n",
    "\n",
    "def spearmanr_pval(x,y):\n",
    "    return spearmanr(x,y)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(method=spearmanr_pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8943bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(round(df.corr(),2), cmap = \"coolwarm\", annot = True, annot_kws = {\"size\":10});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ed210",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='sentiment', y='views', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_lose = df_0.query('sentiment==1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38023468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_win = df_10.query('sentiment==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_pos.views, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe76d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = df_4.query('sentiment==1')\n",
    "df_positive.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e62a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative = df_6.query('sentiment==-0')\n",
    "df_negative.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(round(df_6.corr(),2), cmap = \"coolwarm\", annot = True, annot_kws = {\"size\":10});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trim = df[['title', 'views', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83384c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(round(df_trim.corr(),2), cmap = \"coolwarm\", annot = True, annot_kws = {\"size\":10});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df.query('0<views<100')\n",
    "df_1 = df.query('0<views<1000') \n",
    "df_2 = df.query('1000<views<10_000')\n",
    "df_3 = df.query('10_000<views<100_000') \n",
    "df_4 = df.query('100_000<views< 500_000') \n",
    "df_5 = df.query('500_000<views<1_000_000') \n",
    "df_6 = df.query('500_000<views<1_000_000') \n",
    "df_7 = df.query('1_000_000<views<10_000_000') \n",
    "df_8 = df.query('10_000_000<views<50_000_000') \n",
    "df_9 = df.query('50_000_000<views<100_000_000') \n",
    "df_10 = df.query('100_000_000<views<1000_000_000') \n",
    "\n",
    "fig, axes = plt.subplots(4,3, figsize=(15,15))\n",
    "\n",
    "ax0 = axes[3,1]\n",
    "ax0.hist(df_0.views, bins=40)\n",
    "ax0.set_xlabel('views')\n",
    "ax0.set_ylabel('count')\n",
    "ax0.set_title('0-100 views')\n",
    "# plt.xticks(df_0.views, range(0, 100, 10))\n",
    "\n",
    "ax1 = axes[0,0]\n",
    "ax1.hist(df_1.views, bins=40)\n",
    "ax1.set_xlabel('views')\n",
    "ax1.set_ylabel('count')\n",
    "\n",
    "ax2 = axes[0,1]\n",
    "ax2.hist(df_2.views, bins=40)\n",
    "ax2.set_xlabel('views')\n",
    "ax2.set_ylabel('count')\n",
    "\n",
    "ax3 = axes[0,2]\n",
    "ax3.hist(df_3.views, bins=40)\n",
    "ax3.set_xlabel('views')\n",
    "ax3.set_ylabel('count')\n",
    "\n",
    "ax4 = axes[1,0]\n",
    "ax4.hist(df_4.views, bins=40)\n",
    "ax4.set_xlabel('views')\n",
    "ax4.set_ylabel('count')\n",
    "\n",
    "ax5 = axes[1,1]\n",
    "ax5.hist(df_5.views, bins=40)\n",
    "ax5.set_xlabel('views')\n",
    "ax5.set_ylabel('count')\n",
    "\n",
    "ax6 = axes[1,2]\n",
    "ax6.hist(df_6.views, bins=40)\n",
    "ax6.set_xlabel('views')\n",
    "ax6.set_ylabel('count')\n",
    "\n",
    "ax7 = axes[2,0]\n",
    "ax7.hist(df_7.views, bins=40)\n",
    "ax7.set_xlabel('views')\n",
    "ax7.set_ylabel('count')\n",
    "\n",
    "ax8 = axes[2,1]\n",
    "ax8.hist(df_8.views, bins=40)\n",
    "ax8.set_xlabel('views')\n",
    "ax8.set_ylabel('count')\n",
    "\n",
    "ax9 = axes[2,2]\n",
    "ax9.hist(df_9.views, bins=40)\n",
    "ax9.set_xlabel('views')\n",
    "ax9.set_ylabel('count')\n",
    "\n",
    "ax10 = axes[3,0]\n",
    "ax10.hist(df_10.views, bins=40)\n",
    "ax10.set_xlabel('views')\n",
    "ax10.set_ylabel('count');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0dd6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.query('0<views< 1000') \n",
    "df_2 = df.query('1000<views<10_000')\n",
    "df_3 = df.query('10_000<views<100_000') \n",
    "df_4 = df.query('100_000<views< 500_000') \n",
    "df_5 = df.query('500_000<views<1_000_000') \n",
    "df_6 = df.query('500_000<views<1_000_000') \n",
    "df_7 = df.query('1_000_000<views<10_000_000') \n",
    "df_8 = df.query('10_000_000<views<50_000_000') \n",
    "df_9 = df.query('50_000_000<views<100_000_000') \n",
    "df_10 = df.query('100_000_000<views<1000_000_000') \n",
    "fig, axes = plt.subplots(4,3, figsize=(15,15))\n",
    "sns.kdeplot(df_1.views, ax=axes[0,0])\n",
    "sns.kdeplot(df_2.views, ax=axes[0,1])\n",
    "sns.kdeplot(df_3.views, ax=axes[0,2])\n",
    "sns.kdeplot(df_4.views, ax=axes[1,0])\n",
    "sns.kdeplot(df_5.views, ax=axes[1,1])\n",
    "sns.kdeplot(df_6.views, ax=axes[1,2])\n",
    "sns.kdeplot(df_7.views, ax=axes[2,0])\n",
    "sns.kdeplot(df_8.views, ax=axes[2,1])\n",
    "sns.kdeplot(df_9.views, ax=axes[2,2])\n",
    "sns.kdeplot(df_10.views, ax=axes[3,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80481cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mil = df.query('500_000<views<1_000_000')\n",
    "plt.hist(df_mil.views, bins=100);\n",
    "df_mil.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.views, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad454912",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_sent_dict = {}\n",
    "for i in df.title:\n",
    "    emotion = NRCLex(i)\n",
    "    # print(f'TITLE: {i}\\n')\n",
    "    # print(f'affect dictionary: {emotion.affect_dict}\\n')\n",
    "    # print(f'words: {emotion.words}\\n')\n",
    "    # print(f'sentences: {emotion.sentences}\\n')\n",
    "    # print(f'affect list: {emotion.affect_list}\\n')\n",
    "    # print(f'raw emotion score: {emotion.raw_emotion_scores}\\n')\n",
    "    # print(f'top emotions: {emotion.top_emotions}\\n')\n",
    "    # print(f'affect frequency: {emotion.affect_frequencies}\\n')\n",
    "    # basic_sent_dict[i] = emotion.affect_frequencies\n",
    "    if 'negative' in emotion.raw_emotion_scores:\n",
    "        basic_sent_dict[i] = 0\n",
    "    elif 'positive' in emotion.raw_emotion_scores:\n",
    "        basic_sent_dict[i] = 1\n",
    "    else:\n",
    "        basic_sent_dict[i] = 0.5\n",
    "# print(basic_sent_dict)\n",
    "        \n",
    "    \n",
    "    \n",
    "df_basic_sent = pd.DataFrame.from_dict(basic_sent_dict, orient = \"index\", columns = ['sentiment']).reset_index()\n",
    "\n",
    "df_basic_sent.rename(columns = {'index':'title'}, inplace = True)\n",
    "\n",
    "df = df.merge(df_basic_sent, on='title')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1370e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner_and_splitter(text):\n",
    "    return (text.lower()\n",
    "            .replace('â¤ï¸', ' heart ').replace('ðŸ’¯', ' 100 ').replace('â¤', ' heart ')\n",
    "            .replace('ðŸ™', ' pray ').replace('ðŸ˜˜', ' kiss ').replace('ðŸ¤—', ' happy ')\n",
    "            .replace('ðŸ’¥', ' boom ').replace('âœ”ï¸', ' like ').replace('ðŸ˜', ' love ')\n",
    "            .replace('ðŸ±', ' cat ').replace('ðŸ’”', ' broken heart ').replace('ðŸ˜µ', ' confused ') \n",
    "            .replace('ðŸ˜„', ' awesome ').replace('ðŸ‘', ' thumbs up ').replace('ðŸ˜Ž', ' cool ')\n",
    "            .replace('ðŸ·', ' pig ').replace('ðŸ¤˜', \" rock'n roll \").replace('ðŸ¤£', ' laughing hard ')\n",
    "            .replace('ðŸ˜©', ' oh no ').replace('ðŸ’Ž', ' diamond ').replace('ðŸ˜Š', ' nice ')\n",
    "            .replace('â˜ºï¸', ' very nice ').replace('ðŸ™ƒ', ' upside down smile ').replace('ðŸ¤”', ' not sure ')\n",
    "            .replace('ðŸ˜‚', ' laughing ').replace('ðŸ™‹ðŸ»â€â™€ï¸',' hi ').replace('ðŸ¥°', ' lovely ').replace('ðŸ¥º',' sad ')\n",
    "            .replace('!!!!', '!').replace('!!!', '!').replace('!!', '!').replace('!',' ! ')\n",
    "            .replace('????', '?').replace('???', '?').replace('??', '?').replace('?',' ? ')\n",
    "            .replace('oooo','o').replace('oooo','o').replace('ooo','o')\n",
    "            .replace('..','...').replace('......','...').replace('....','...').replace('....','...').replace('...',' ... ')\n",
    "            .replace('.',' . ').replace('. . .', ' ... ')\n",
    "            .replace(\"'\", \" \").replace('\"', ' ')\n",
    "            .replace('    ', ' ').replace('   ', ' ').replace('  ', ' ')\n",
    "            .split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d797d",
   "metadata": {},
   "source": [
    "So priorities are \n",
    "- sentiment analysis optimization\n",
    "- extracting titles\n",
    "- running analysis with those features extracted\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
